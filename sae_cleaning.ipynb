{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import plotly.express as px\n",
    "import torch as t\n",
    "import pandas as pd\n",
    "from tools.globals import load_country_globals\n",
    "\n",
    "from translate import Translator\n",
    "\n",
    "from nnsight import LanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "from tabulate import tabulate\n",
    "\n",
    "from tools.nnsight_utils import collect_residuals, visualize_top_tokens\n",
    "from tools.patchscope import patch_scope_gen\n",
    "from tools.sae import display_dashboard\n",
    "\n",
    "load_country_globals()\n",
    "translator = Translator(from_lang=\"autodetect\",to_lang=\"en\")\n",
    "\n",
    "device = t.device(\n",
    "    \"mps\" if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "load_dotenv()\n",
    "t.set_grad_enabled(False)\n",
    "\n",
    "t.manual_seed(42)\n",
    "if t.cuda.is_available():\n",
    "    t.cuda.manual_seed_all(42)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b8bffc7a9741e4a516dbf322c8a3b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-9b-it into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from sae_lens import HookedSAETransformer, SAE\n",
    "\n",
    "model = HookedSAETransformer.from_pretrained('gemma-2-9b-it', \n",
    "                                                        device=device,\n",
    "                                                        torch_dtype=t.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_release = \"gemma-scope-9b-pt-res-canonical\"\n",
    "sae_id = \"layer_22/width_131k/canonical\"\n",
    "\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(release=sae_release, sae_id=sae_id, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2797/2533723538.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  res_tr = t.load(f\"residuals/gemma2_9b_it/{prefix}_en_tr_hint_bal.pt\")\n",
      "/tmp/ipykernel_2797/2533723538.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  res_en = t.load(f\"residuals/gemma2_9b_it/{prefix}_en_no_hint_bal.pt\")\n"
     ]
    }
   ],
   "source": [
    "prefix = \"tr\"\n",
    "\n",
    "res_tr = t.load(f\"residuals/gemma2_9b_it/{prefix}_en_tr_hint_bal.pt\")\n",
    "res_en = t.load(f\"residuals/gemma2_9b_it/{prefix}_en_no_hint_bal.pt\")\n",
    "\n",
    "steering_vec = (res_tr - res_en).mean(dim=0)\n",
    "steering_vec = steering_vec.unsqueeze(1)\n",
    "\n",
    "original_vec = steering_vec[22,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent 9718 had activation 172.12\n",
      "Latent 129153 had activation 14.69\n",
      "Latent 62653 had activation 13.07\n",
      "Latent 53185 had activation 10.85\n",
      "Latent 15271 had activation 10.58\n",
      "Latent 5747 had activation 10.43\n",
      "Latent 114557 had activation 10.30\n",
      "Latent 41951 had activation 10.10\n",
      "Latent 15986 had activation 9.27\n",
      "Latent 72652 had activation 8.61\n",
      "Latent 117451 had activation 8.23\n",
      "Latent 84883 had activation 8.19\n",
      "Latent 28597 had activation 7.95\n",
      "Latent 67587 had activation 7.88\n",
      "Latent 78561 had activation 7.74\n",
      "Latent 99416 had activation 7.38\n",
      "Latent 47353 had activation 7.33\n",
      "Latent 58799 had activation 7.29\n",
      "Latent 82729 had activation 7.18\n",
      "Latent 35449 had activation 7.18\n",
      "Latent 45734 had activation 6.74\n",
      "Latent 130487 had activation 6.65\n",
      "Latent 84417 had activation 6.23\n",
      "Latent 59907 had activation 6.09\n",
      "Latent 118549 had activation 6.03\n",
      "Latent 63281 had activation 6.02\n",
      "Latent 128932 had activation 6.01\n",
      "Latent 38741 had activation 5.83\n",
      "Latent 77228 had activation 5.74\n",
      "Latent 74918 had activation 5.71\n",
      "Latent 45785 had activation 5.50\n",
      "Latent 125119 had activation 5.39\n",
      "Latent 46554 had activation 5.26\n",
      "Latent 10238 had activation 5.25\n",
      "Latent 98210 had activation 5.23\n",
      "Latent 21706 had activation 5.21\n",
      "Latent 104236 had activation 5.18\n",
      "Latent 43662 had activation 5.15\n",
      "Latent 105209 had activation 5.15\n",
      "Latent 66332 had activation 5.13\n",
      "Latent 94452 had activation 5.10\n",
      "Latent 21799 had activation 5.09\n",
      "Latent 68755 had activation 5.06\n",
      "Latent 123978 had activation 5.06\n",
      "Latent 109700 had activation 5.01\n",
      "Latent 92713 had activation 4.99\n",
      "Latent 46218 had activation 4.96\n",
      "Latent 74093 had activation 4.91\n",
      "Latent 63536 had activation 4.91\n",
      "Latent 71361 had activation 4.90\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tools.sae import display_dashboard\n",
    "\n",
    "\n",
    "vec_slice = model.blocks[22].ln2(original_vec)\n",
    "\n",
    "\n",
    "sae_acts_post = sae.encode(original_vec.squeeze(0))\n",
    "#sae_acts_post = sae.encode(vec_slice.squeeze(0))\n",
    "\n",
    "# Plot the latent activations\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sae_acts_post.cpu().numpy())\n",
    "plt.title(f\"Latent activations at the final token position ({sae_acts_post.nonzero().numel()} alive)\")\n",
    "plt.xlabel(\"Latent\")\n",
    "plt.ylabel(\"Activation\")\n",
    "plt.show()\n",
    "\n",
    "# Print the top 3 latents, and inspect their dashboards\n",
    "top_activations, top_indices = sae_acts_post.topk(50, largest=True)\n",
    "\n",
    "for act, ind in zip(top_activations, top_indices):\n",
    "    print(f\"Latent {ind.item()} had activation {act.item():.2f}\")\n",
    "    #display_dashboard(latent_idx=ind.item(), sae_release=sae_release, sae_id=sae_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = t.nn.Parameter(sae_acts_post, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def injection_hook(module, input, output):\n",
    "    # output shape is [batch_size, seq_len, hidden_dim]\n",
    "    # decode z\n",
    "    #z = model.blocks[22].ln2(z)\n",
    "    decoded = sae.decode(z)                        # [hidden_dim]\n",
    "    decoded = decoded.unsqueeze(0).unsqueeze(1) # [1,1,hidden_dim]\n",
    "    decoded = decoded.expand(output.size(0), 1, -1)  # [batch_size, 1, hidden_dim]\n",
    "    decoded = t.cat([t.zeros_like(output[:, :-1, :]), decoded], dim=1)  # [batch_size, seq_len, hidden_dim]\n",
    "    return output + 4 * decoded  # broadcast add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.blocks[22].hook_resid_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x70319255a650>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.blocks[22].hook_resid_post.register_forward_hook(injection_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = t.optim.Adam(\n",
    "    [z],\n",
    "    lr=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"data/final_data_filtered.csv\")\n",
    "prompt_suffix = {\n",
    "    \"English\": \"My guess is **\",\n",
    "    \"Turkish\": \"Tahminim **\",\n",
    "    \"French\": \"Ma supposition est **\",\n",
    "    \"Russian\": \"Моё предположение **\",\n",
    "    \"Bengali\": \"আমার অনুমান হলো **\",\n",
    "}\n",
    "\n",
    "df = data_df.query(\"country=='Turkey' and hint=='none'\").copy()\n",
    "\n",
    "df[\"text\"] = data_df.apply(lambda x: f\"{x['input']}{prompt_suffix[x['lang']]}\", axis=1)\n",
    "df[\"label\"] = data_df[\"ans_local\"].apply(lambda x: x+\"**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyTextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = row[\"text\"]\n",
    "        label = row[\"label\"]  # integer class\n",
    "        # tokenize\n",
    "        tokenized = self.tokenizer(\n",
    "            text, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\",\n",
    "            add_special_tokens=False,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # for classification, we might not need to shift labels as in next-token LM\n",
    "        return {\n",
    "            \"input_ids\": tokenized[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": tokenizer(label, add_special_tokens=False)\n",
    "        }\n",
    "\n",
    "dataset = MyTextDataset(df.sample(400), tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = t.nn.CrossEntropyLoss()\n",
    "\n",
    "def lm_loss_function(logits, targets):\n",
    "    # logits: [batch_size, seq_len, vocab_size]\n",
    "    # targets: [batch_size, seq_len]\n",
    "    # Typically we shift targets by 1 for next-token prediction, etc.\n",
    "    return cross_entropy(logits, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_penalty_z(z):\n",
    "    # z is [latent_dim]\n",
    "    # sum |z_i| over i where mask[i] == True\n",
    "    return (z.abs()).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def alignment_penalty(decoded, ref):\n",
    "    return t.nn.functional.mse_loss(decoded, ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ce = 1.0\n",
    "lambda_l0 = 0.01\n",
    "lambda_align = 0.0\n",
    "\n",
    "def combined_loss_fn(logits, targets, z, reference_vector):\n",
    "    loss_ce = lm_loss_function(logits, targets)\n",
    "    loss_l0 = l1_penalty_z(z)  # or a better approach\n",
    "    decoded = sae.decode(z)\n",
    "    loss_align = alignment_penalty(decoded, reference_vector)\n",
    "    return (lambda_ce * loss_ce\n",
    "            + lambda_l0 * loss_l0\n",
    "            + lambda_align * loss_align)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "latent_dim = sae_acts_post.size(0)\n",
    "\n",
    "# Example mask: 1 = trainable, 0 = frozen\n",
    "trainable_mask = sae_acts_post > 0\n",
    "\n",
    "def zero_grad_frozen_dims(param, mask):\n",
    "    \"\"\"\n",
    "    In-place zero out gradient for positions we want to freeze.\n",
    "    This ensures those positions don't update even if they have gradients.\n",
    "    \"\"\"\n",
    "    if param.grad is not None:\n",
    "        param.grad[~mask] = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1070 [00:00<?, ?it/s]/tmp/ipykernel_1690/1713747666.py:2: UserWarning: Using a target size (torch.Size([1, 3584])) that is different to the input size (torch.Size([3584])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return t.nn.functional.mse_loss(decoded, ref)\n",
      "100%|██████████| 1070/1070 [02:35<00:00,  6.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 finished. Last batch loss = 18.7556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1070/1070 [02:31<00:00,  7.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished. Last batch loss = 22.2556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 308/1070 [00:43<01:47,  7.11it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 30\u001b[0m\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Forward pass => injection_hook => hidden + decoder(z)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# We'll assume the model outputs classification logits\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# For some HuggingFace models, logits might be in outputs.logits\u001b[39;00m\n\u001b[1;32m     32\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs  \u001b[38;5;66;03m# [batch_size, num_classes]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/default/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/default/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/envs/default/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:576\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    572\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    573\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    574\u001b[0m         )\n\u001b[0;32m--> 576\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m/opt/conda/envs/default/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/default/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/envs/default/lib/python3.11/site-packages/transformer_lens/components/transformer_block.py:160\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m    153\u001b[0m     key_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    154\u001b[0m     value_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    156\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_normalization_before_and_after:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# If we use LayerNorm both before and after, then apply the second LN after the layer\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# and before the hook. We do it before the hook so hook_attn_out captures \"that which\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# is added to the residual stream\"\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1_post(attn_out)\n",
      "File \u001b[0;32m/opt/conda/envs/default/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/default/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/envs/default/lib/python3.11/site-packages/transformer_lens/components/abstract_attention.py:252\u001b[0m, in \u001b[0;36mAbstractAttention.forward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask, position_bias)\u001b[0m\n\u001b[1;32m    249\u001b[0m     attn_scores \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_bias\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mattention_dir \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcausal\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m# If causal attention, we mask it to only attend backwards. If bidirectional, we don't mask.\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m     attn_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_causal_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache_pos_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, head_index, query_pos, key_pos]\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m additive_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    256\u001b[0m     attn_scores \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m additive_attention_mask\n",
      "File \u001b[0;32m/opt/conda/envs/default/lib/python3.11/site-packages/transformer_lens/components/abstract_attention.py:461\u001b[0m, in \u001b[0;36mAbstractAttention.apply_causal_mask\u001b[0;34m(self, attn_scores, past_kv_pos_offset, attention_mask)\u001b[0m\n\u001b[1;32m    459\u001b[0m     einsum_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch head pos offset_pos, batch offset_pos -> batch head pos offset_pos\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    460\u001b[0m     final_mask \u001b[38;5;241m=\u001b[39m final_mask\u001b[38;5;241m.\u001b[39mto(attention_mask\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 461\u001b[0m     final_mask \u001b[38;5;241m=\u001b[39m \u001b[43meinops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meinsum_str\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mbool()\n\u001b[1;32m    463\u001b[0m attn_scores \u001b[38;5;241m=\u001b[39m attn_scores\u001b[38;5;241m.\u001b[39mto(final_mask\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mwhere(final_mask, attn_scores, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mIGNORE)\n",
      "File \u001b[0;32m/opt/conda/envs/default/lib/python3.11/site-packages/einops/einops.py:900\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*tensors_and_pattern)\u001b[0m\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    897\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe last argument passed to `einops.einsum` must be a string, representing the einsum pattern.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    898\u001b[0m     )\n\u001b[1;32m    899\u001b[0m tensors \u001b[38;5;241m=\u001b[39m tensors_and_pattern[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 900\u001b[0m pattern \u001b[38;5;241m=\u001b[39m \u001b[43m_compactify_pattern_for_einsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_backend(tensors[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39meinsum(pattern, \u001b[38;5;241m*\u001b[39mtensors)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Put decoder & z on the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#decoder = decoder.to(device)\n",
    "z = nn.Parameter(z.to(device))\n",
    "\n",
    "reference_vector = original_vec.to(device)  # shape [hidden_dim]\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    [z], \n",
    "    lr=1e-3\n",
    ")\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "model.eval()  # The LLM is frozen anyway; if there's dropout or LayerNorm, you might keep it in eval\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        label = batch[\"label\"][\"input_ids\"][-1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass => injection_hook => hidden + decoder(z)\n",
    "        # We'll assume the model outputs classification logits\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask).to(device)\n",
    "        # For some HuggingFace models, logits might be in outputs.logits\n",
    "        logits = outputs  # [batch_size, num_classes]\n",
    "\n",
    "        # Compute combined loss\n",
    "        loss = combined_loss_fn(logits[:,-1,:], label, z, reference_vector)\n",
    "\n",
    "        loss.requires_grad = True\n",
    "        loss.backward()\n",
    "\n",
    "        # Zero grad in the frozen dims of z\n",
    "        #zero_grad_frozen_dims(z, trainable_mask)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch} finished. Last batch loss = {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([172.1247,  14.6867,  13.0734,  10.8545,  10.5843,  10.4345,  10.2991,\n",
       "         10.0962,   9.2711,   8.6055,   8.2304,   8.1877,   7.9508,   7.8799,\n",
       "          7.7355,   7.3832,   7.3284,   7.2882,   7.1823,   7.1800,   6.7441,\n",
       "          6.6518,   6.2257,   6.0919,   6.0255,   6.0239,   6.0124,   5.8300,\n",
       "          5.7418,   5.7088,   5.5028,   5.3875,   5.2589,   5.2533,   5.2270,\n",
       "          5.2143,   5.1799,   5.1547,   5.1454,   5.1334,   5.0992,   5.0874,\n",
       "          5.0643,   5.0615,   5.0140,   4.9867,   4.9637,   4.9062,   4.9061,\n",
       "          4.8974], device='cuda:0'),\n",
       "indices=tensor([  9718, 129153,  62653,  53185,  15271,   5747, 114557,  41951,  15986,\n",
       "         72652, 117451,  84883,  28597,  67587,  78561,  99416,  47353,  58799,\n",
       "         82729,  35449,  45734, 130487,  84417,  59907, 118549,  63281, 128932,\n",
       "         38741,  77228,  74918,  45785, 125119,  46554,  10238,  98210,  21706,\n",
       "        104236,  43662, 105209,  66332,  94452,  21799,  68755, 123978, 109700,\n",
       "         92713,  46218,  74093,  63536,  71361], device='cuda:0'))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae_acts_post.topk(50, largest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(131072, device='cuda:0')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sae_acts_post == z).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([172.1247,  14.6867,  13.0734,  10.8545,  10.5843,  10.4345,  10.2991,\n",
       "         10.0962,   9.2711,   8.6055,   8.2304,   8.1877,   7.9508,   7.8799,\n",
       "          7.7355,   7.3832,   7.3284,   7.2882,   7.1823,   7.1800,   6.7441,\n",
       "          6.6518,   6.2257,   6.0919,   6.0255,   6.0239,   6.0124,   5.8300,\n",
       "          5.7418,   5.7088,   5.5028,   5.3875,   5.2589,   5.2533,   5.2270,\n",
       "          5.2143,   5.1799,   5.1547,   5.1454,   5.1334,   5.0992,   5.0874,\n",
       "          5.0643,   5.0615,   5.0140,   4.9867,   4.9637,   4.9062,   4.9061,\n",
       "          4.8974], device='cuda:0'),\n",
       "indices=tensor([  9718, 129153,  62653,  53185,  15271,   5747, 114557,  41951,  15986,\n",
       "         72652, 117451,  84883,  28597,  67587,  78561,  99416,  47353,  58799,\n",
       "         82729,  35449,  45734, 130487,  84417,  59907, 118549,  63281, 128932,\n",
       "         38741,  77228,  74918,  45785, 125119,  46554,  10238,  98210,  21706,\n",
       "        104236,  43662, 105209,  66332,  94452,  21799,  68755, 123978, 109700,\n",
       "         92713,  46218,  74093,  63536,  71361], device='cuda:0'))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.topk(50, largest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 65, 256000])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-26.1250,  17.3750,  21.2500,  ..., -10.4375,  -3.5781, -13.8750],\n",
       "         [-24.3750,   4.1562, -16.3750,  ..., -13.4375, -10.7500, -25.2500],\n",
       "         [-25.3750,   2.9375, -17.7500,  ..., -14.4375, -11.4375, -26.1250],\n",
       "         ...,\n",
       "         [-27.1250,  -1.1875,   0.1836,  ..., -12.1250, -15.0000, -26.7500],\n",
       "         [-13.4375,   0.1699,  -4.4375,  ...,  -5.7188,  -5.8750, -13.6250],\n",
       "         [-14.1250,  -1.3047,   6.3438,  ...,  -7.4062,  -7.5000, -15.6875]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[\"input_ids\"][-1].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [tensor([5523]), tensor([14132]), tensor([23398]), tensor([8155]), tensor([688])], 'attention_mask': [tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1])]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 256000])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
